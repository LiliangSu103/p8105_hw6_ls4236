---
title: "p8105_hw6_ls4236"
author: "Liliang Su"
date: "2025-11-16"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(modelr)


knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Load and clean data.

```{r}
homi_df_raw = 
  read_csv(file = "./data/homicide-data.csv", na = c("NA",".","")) |> 
  janitor::clean_names() 

homi_df = homi_df_raw |> 
  mutate(
    city_state = str_c(city, ", ", state)
  ) |> 
  # binary indicator
  mutate(resolved = disposition == "Closed by arrest") |>
  # Omit specified cities
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) |>
  # Limit analysis to 'white' or 'black' victims
  filter(victim_race %in% c("White", "Black")) |>
  # make sure victim_age is numeric
  mutate(victim_age = as.numeric(victim_age)) |> 
  mutate(victim_sex = factor(victim_sex, levels = c("Female", "Male")),
         victim_race = factor(victim_race, levels = c("Black", "White")))
```

### Baltimore GLM Analysis

```{r}
baltimore_df <- homi_df |>
  filter(city_state == "Baltimore, MD")

# Fit the logistic regression model
baltimore_model <- glm(
  resolved ~ victim_age + victim_sex + victim_race,
  family = binomial(link = "logit"),
  data = baltimore_df
)

# Apply tidy, get estimate and CIs for adjusted ORs
baltimore_results <- broom::tidy(baltimore_model, conf.int = TRUE, exponentiate = TRUE) |>
  # Filter for the Male coefficient (Male is level 2, Female is reference)
  filter(term == "victim_sexMale") |>
  select(estimate, conf.low, conf.high)

baltimore_results
```

The estimate of adjusted odds ratio for for solving homicides comparing male victims to female victims keeping all other variables fixed is `r baltimore_results[[1]]`, and CI is (`r baltimore_results[[2]]`, `r baltimore_results[[3]]`).


### Multi-cities GLM Analysis

```{r}
# function to fit the model and extract the Male OR
glm_for_maleOR <- function(data) {
  
  # fit the model
  fit = glm(
    resolved ~ victim_age + victim_sex + victim_race,
    family = binomial(link = "logit"),
    data = data
    )
  
  # Extract OR and CI using broom
  tidy_results <- broom::tidy(fit, conf.int = TRUE, exponentiate = TRUE) |>
    filter(term == "victim_sexMale") |>
    select(estimate, conf.low, conf.high)
  
  return(tidy_results)
  }

# Run the analysis across all cities
cities_results <- homi_df |>
  group_by(city_state) |>
  nest() |>
  # Apply the function to each city's data
  mutate(tidy_results = map(data, glm_for_maleOR)) |>
  select(city_state, tidy_results) |>
  # Expand the list-column of ORs into rows
  unnest(tidy_results)

cities_results |> 
  knitr::kable()
```

```{r}

# Create the plot
cities_plot <- cities_results |>
  # Reorder city_state factor by the OR estimate
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = estimate, y = city_state)) +
  # Add horizontal error bars for the 95% CI
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2, color = "dodgerblue") +
  # Add points for the OR estimate
  geom_point() +
  # Add a vertical reference line at OR = 1 (no effect)
  geom_vline(xintercept = 1, linetype = "dashed", color = "red") +
  
  # Labels and themes
  labs(
    title = "Adjusted Odds Ratio for Homicide Resolution",
    subtitle = "Comparing Male vs. Female Victims (Adjusted for Age and Race)",
    x = "Adjusted Odds Ratio (Male vs. Female)",
    y = NULL
  ) +
  scale_x_continuous(trans = 'log10', breaks = c(0.25, 0.5, 1, 2, 4)) + 
  theme(
    axis.text.y = element_text(size = 8),
    plot.title = element_text(face = "bold")
  )

cities_plot
```

**Comment**:

- **Interpretation of Estimate**: Most cities have ORs less than 1.0, which means there is a prevalent pattern where homicides involving male victims are less likely to be solved than those involving female victims. For example, cities like New York shows a strong, statistically significant difference, where the odds of solving a male victim's homicide are less than half that of a female victim's.

- **Interpretation of CIs**: The width of confidence intervals indicates the statistical certainty of the adujsted ORs. For many cities, the horizontal CI bar crosses the red line at OR = 1.0. In these cities, we do not have significant evidence to conclude that victim sex is a significant predictor of homicide resolution, when controlling for age and race. On the other hand, there are quite a few cities confidence interval upper bounds below 1, which means there are significant evidence that victim sex is a significant predictor of homicide resolution, when controlling for age and race.

- **Cities with Higher Odds**: A few cities like Albuquerque shows an estimated OR greater than 1.0, though the confidence interval is often wide. This suggests the possibility that male victims' homicides are more likely to be solved there, but the uncertainty is high.

## Problem 2

Load dataset.

```{r}
data("weather_df")
  
weather_df = weather_df |>
  select(tmax, tmin, prcp) |>
  drop_na()
```

### Bootstrap

Let's write a function to extract required estimates.

```{r}
# A single function to perform the full extraction for one bootstrap sample
lm_fit = function(data) {
  fit = lm(tmax ~ tmin + prcp, data = data)
  # Extract model statistics (R^2)
  r_sq = broom::glance(fit) |> 
    pull(r.squared)
  # Extract term statistics (coefficients)
  betas = broom::tidy(fit) |>
    filter(term %in% c("tmin", "prcp")) |>
    pull(estimate)
  
  # 4. Calculate the ratio (tmin / prcp)
  beta_ratio = betas[1] / betas[2]
  
  tibble(
    r_hat_squared = r_sq,
    beta_hat_ratio = beta_ratio
  )
}
```

Fit with 5000 bootstrap samples.

```{r}
set.seed(1)
# Fit and Store the results
bootstrap_results = weather_df |> 
  bootstrap(n = 5000) |> 
  mutate(
    df = map(strap, as_tibble),
    results = map(df, lm_fit),
    ) |> 
  select(.id, results) |> 
  unnest(results)
  
```


### Visualization of Distribution

```{r}
# Plot for R^2
r_sq_plot <- bootstrap_results |>
  ggplot(aes(x = r_hat_squared)) +
  geom_histogram(bins = 50, fill = "mediumpurple", color = "white") +
  geom_vline(xintercept = quantile(pull(bootstrap_results, r_hat_squared), c(0.025, 0.975)), linetype = "dashed", color = "red") +
  labs(
    title = expression(paste("Bootstrap Distribution of ", hat(r)^2)),
    x = expression(hat(r)^2),
    y = "Frequency"
  ) 

r_sq_plot
```

**Interpretation**: The distribution of bootstrapped coefficient of determination $\hat{r}^2$ is unimodal, symmetrical and highly concentrated, centered around approximately 0.942, though with a subtle left skew. This tight grouping of estimates, confirmed by the narrow 95% confidence interval, indicates that the model has very high and stable explanatory power. We can be highly confident that the true explanatory power of these two predictors falls in this narrow range, consistently explaining well over 93% of the variation in the maximum temperature.

```{r}
# Plot for Beta Product
beta_product_plot <- bootstrap_results |>
  ggplot(aes(x = beta_hat_ratio)) +
  geom_histogram(bins = 50, fill = "lightgreen", color = "white") +
  geom_vline(xintercept = quantile(pull(bootstrap_results, beta_hat_ratio), c(0.025, 0.975)), linetype = "dashed", color = "red") +
  labs(
    title = expression(paste("Bootstrap Distribution of ", hat(beta)[1] / hat(beta)[2])),
    x = expression(hat(beta)[1] / hat(beta)[2]),
    y = "Frequency"
  ) 

beta_product_plot
```

**Interpretation**: The distribution of the $\frac{\hat{\beta_1}}{\hat{\beta_2}}$ is unimodal and clearly left-skewed, with its peak shifted toward the right end of the distribution and a long tail extending to the left (negative values). The 95% confidence interval consists entirely of large negative values and does not include zero. This leads to two strong conclusions:

- Magnitude: The effect of minimum temperature `tmin` on maximum temperature is reliably estimated to be hundreds of times greater than the effect of precipitation `prcp`.

- Direction: The negative sign throughout the distribution confirms that the true effects of the two predictors on maximum temperature have opposite signs.

### 95% Confidence Interval

```{r}
# Calculate 95% CI for both parameters
ci_summary <- bootstrap_results |>
  summarise(
    R_Squared_LCI = quantile(r_hat_squared, 0.025),
    R_Squared_UCI = quantile(r_hat_squared, 0.975),
    Beta_ratio_LCI = quantile(beta_hat_ratio, 0.025),
    Beta_ratio_UCI = quantile(beta_hat_ratio, 0.975)
  )
```

- The 95% CI for $\hat{R}^2$ is (`r ci_summary[[1]][1]`, `r ci_summary[[2]][1]`). We are 95% confident that the true proportion of variance in maximum temperature `tmax` that is jointly explained by minimum temperature `tmin` and precipitation `prcp` lies within the calculated interval.

- The The 95% CI for $\frac{\hat{\beta_1}}{\hat{\beta_2}}$ is (`r ci_summary[[3]][1]`, `r ci_summary[[4]][1]`). We are 95% confident that the true ratio of the effect of `tmin` to the effect of `prcp` lies within the calculated interval.
